{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca92a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습문제 1 : 네이버 사이트 자동 검색하기\n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time   \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 네이버 사이트의 자동 검색용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.수집할 자료의 키워드는 무엇입니까?(예: 서진수 빅데이터): ')\n",
    "print(\"\\n\")\n",
    "\n",
    "#Step 3. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://www.naver.com/'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "driver.maximize_window()\n",
    "\n",
    "#Step 4. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'query')\n",
    "driver.find_element(By.ID,'query').click( )\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "#Step 5.뉴스 선택하기\n",
    "driver.find_element(By.LINK_TEXT,'뉴스').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 6.Beautiful Soup 로 본문 내용만 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "html_1 = driver.page_source #현재 페이지의 전체 소스코드를 다 가져오기\n",
    "soup_1 = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "content_1 = soup_1.find('ul','list_news').find_all('li')\n",
    "for i in content_1 :\n",
    "    print(i.get_text().replace(\"\\n\",\" \").strip())\n",
    "    print(\"\\n\")\n",
    "\n",
    "#Step 7. 표준 출력 방향을 바꾸어 txt 파일에 저장하기\n",
    "import sys \n",
    "f_name = input('결과를 저장할 파일명을 쓰세요(예: c:\\\\py_temp\\\\naver_news.txt): ')\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "file = open(f_name , 'a' , encoding='UTF-8')\n",
    "sys.stdout = file  #모니터에 출력하지 말고 file 에 출력해라\n",
    "\n",
    "for i in content_1 :\n",
    "    print(i.get_text().replace(\"\\n\",\"\"))\n",
    "\n",
    "file.close()    \n",
    "sys.stdout = orig_stdout  #원래대로 변경 - 다시 화면에 출력시켜라    \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name)\n",
    "\n",
    "#Step 8. open -> write -> close 방식으로  txt 파일에 저장하기\n",
    "f_name_2 = 'c:\\\\py_temp\\\\naver_news_2.txt' \n",
    "\n",
    "file = open(f_name_2 , 'a' , encoding='UTF-8')\n",
    "\n",
    "for i in content_1 :\n",
    "    file.write(i.get_text()+'\\n')\n",
    "\n",
    "file.close()      \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습문제 2 : 다음 사이트 자동 검색하기\n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time   \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 다음 사이트의 자동 검색용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.수집할 자료의 키워드는 무엇입니까?(예: 서진수 빅데이터): ')\n",
    "print(\"\\n\")\n",
    "\n",
    "#Step 3. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://www.daum.net/'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "driver.maximize_window()\n",
    "\n",
    "#Step 4. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'q')\n",
    "driver.find_element(By.ID,'q').click( )\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "#Step 5.뉴스 선택하기\n",
    "driver.find_element(By.LINK_TEXT,'뉴스').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 6.Beautiful Soup 로 본문 내용만 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "html_1 = driver.page_source #현재 페이지의 전체 소스코드를 다 가져오기\n",
    "soup_1 = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "content_1 = soup_1.find('ul','list_news').find_all('li')\n",
    "for i in content_1 :\n",
    "    print(i.get_text().replace(\"\\n\",\" \").strip())\n",
    "    print(\"\\n\")\n",
    "\n",
    "#Step 7. 표준 출력 방향을 바꾸어 txt 파일에 저장하기\n",
    "import sys \n",
    "f_name = input('결과를 저장할 파일명을 쓰세요(예: c:\\\\py_temp\\\\daum_news.txt): ')\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "file = open(f_name , 'a' , encoding='UTF-8')\n",
    "sys.stdout = file  #모니터에 출력하지 말고 file 에 출력해라\n",
    "\n",
    "for i in content_1 :\n",
    "    print(i.get_text().replace(\"\\n\",\"\"))\n",
    "\n",
    "file.close()    \n",
    "sys.stdout = orig_stdout  #원래대로 변경 - 다시 화면에 출력시켜라    \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab11e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8. open -> write -> close 방식으로  txt 파일에 저장하기\n",
    "f_name_2 = 'c:\\\\py_temp\\\\daum_news_2.txt' \n",
    "\n",
    "file = open(f_name_2 , 'a' , encoding='UTF-8')\n",
    "\n",
    "for i in content_1 :\n",
    "    file.write(i.get_text()+'\\n')\n",
    "\n",
    "file.close()      \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#연습문제 3: 대한민국 구석구석사이트 자동 검색하기\n",
    "    \n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 대한민국 구석구석 사이트의 자동 검색용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.수집할 자료의 키워드는 무엇입니까?(예: 제주도): ')\n",
    "print(\"\\n\")\n",
    "\n",
    "#Step 3. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://korean.visitkorea.or.kr'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "driver.maximize_window()\n",
    "\n",
    "# 팝업 안내창 모두 닫기\n",
    "main = driver.window_handles \n",
    "for handle in main: \n",
    "    if handle != main[0]: \n",
    "        driver.switch_to_window(handle) \n",
    "        driver.close()\n",
    "\n",
    "# 원래 창으로 돌아가기\n",
    "driver.switch_to.window(driver.window_handles[0])    \n",
    "       \n",
    "#Step 4. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'inp_search')\n",
    "driver.find_element(By.ID,'inp_search').click( )\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 방법\n",
    "from bs4 import BeautifulSoup\n",
    "time.sleep(1)\n",
    "full_html = driver.page_source\n",
    "soup = BeautifulSoup(full_html, 'html.parser')\n",
    "\n",
    "content_list = soup.find('div','area_sWordList').get_text()\n",
    "print(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 방법\n",
    "\n",
    "time.sleep(1)\n",
    "full_html = driver.page_source\n",
    "soup = BeautifulSoup(full_html, 'html.parser')\n",
    "content_list = soup.find('div','area_sWordList').find_all('li')\n",
    "\n",
    "for i in content_list:\n",
    "    print(i.get_text())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세번째 방법\n",
    "\n",
    "time.sleep(1)\n",
    "full_html = driver.page_source\n",
    "soup = BeautifulSoup(full_html, 'html.parser')\n",
    "content_list = soup.find('div','area_sWordList').find_all('li')\n",
    "\n",
    "for i in content_list:\n",
    "    txt = i.find('em').get_text()\n",
    "    print(txt)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#표준 출력 방향을 바꾸어 txt 파일에 저장하기\n",
    "import sys \n",
    "f_name = input('결과를 저장할 파일명을 쓰세요(예: c:\\\\py_temp\\\\korea.txt): ')\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "file = open(f_name , 'a' , encoding='UTF-8')\n",
    "sys.stdout = file  #모니터에 출력하지 말고 file 에 출력해라\n",
    "\n",
    "for i in content_list:\n",
    "    txt = i.find('em').get_text()\n",
    "    print(txt)\n",
    "\n",
    "file.close()    \n",
    "sys.stdout = orig_stdout  #원래대로 변경 - 다시 화면에 출력시켜라    \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb145ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open -> write -> close 방식으로  txt 파일에 저장하기\n",
    "f_name_2 = 'c:\\\\py_temp\\\\korea_2.txt' \n",
    "\n",
    "file = open(f_name_2 , 'a' , encoding='UTF-8')\n",
    "\n",
    "for i in content_list :\n",
    "    file.write(i.find('em').get_text()+'\\n')\n",
    "\n",
    "file.close()      \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ec89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#연습문제 4: 대한민국 구석구석사이트 자동 검색하여 텍스트 추출 후 저장하기\n",
    "    \n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 대한민국 구석구석 사이트의 자동 검색용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.수집할 자료의 키워드는 무엇입니까?(예: 제주도): ')\n",
    "print(\"\\n\")\n",
    "\n",
    "#Step 3. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://korean.visitkorea.or.kr'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "driver.maximize_window()\n",
    "\n",
    "# 팝업 안내창 모두 닫기\n",
    "main = driver.window_handles \n",
    "for handle in main: \n",
    "    if handle != main[0]: \n",
    "        driver.switch_to_window(handle) \n",
    "        driver.close()\n",
    "\n",
    "# 원래 창으로 돌아가기\n",
    "driver.switch_to.window(driver.window_handles[0])    \n",
    "       \n",
    "#Step 4. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'inp_search')\n",
    "driver.find_element(By.ID,'inp_search').click( )\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. 검색된 목록을 추출합니다\n",
    "from bs4 import BeautifulSoup\n",
    "full_html = driver.page_source\n",
    "soup = BeautifulSoup(full_html, 'html.parser')\n",
    "\n",
    "content_list = soup.find('ul','list_thumType type1').find_all('li')\n",
    "for a in content_list :\n",
    "    print(a.get_text())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#표준 출력 방향을 바꾸어 txt 파일에 저장하기\n",
    "import sys \n",
    "f_name = input('결과를 저장할 파일명을 쓰세요(예: c:\\\\py_temp\\\\korea_3.txt): ')\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "file = open(f_name , 'a' , encoding='UTF-8')\n",
    "sys.stdout = file  #모니터에 출력하지 말고 file 에 출력해라\n",
    "\n",
    "for a in content_list :\n",
    "    print(a.get_text())\n",
    "    print()\n",
    "\n",
    "file.close()    \n",
    "sys.stdout = orig_stdout  #원래대로 변경 - 다시 화면에 출력시켜라    \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a594b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open -> write -> close 방식으로  txt 파일에 저장하기\n",
    "f_name_2 = 'c:\\\\py_temp\\\\korea_4.txt' \n",
    "\n",
    "file = open(f_name_2 , 'a' , encoding='UTF-8')\n",
    "\n",
    "for i in content_list :\n",
    "    file.write(i.get_text()+'\\n')\n",
    "\n",
    "file.close()      \n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')\n",
    "print('수집된 결과는 %s 에 저장되었습니다' %f_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b272b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
