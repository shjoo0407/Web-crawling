{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " 연습문제 : 네이버 블로그 상세 내역과 댓글 정보 추출하여 저장하기\n",
      "================================================================================\n",
      "1.크롤링할 블로그 주소를 입력하세요: https://blog.naver.com/hy820715/221514204265\n",
      "2.결과 파일을 저장할 폴더명만 쓰세요(예:c:\\py_temp\\):c:\\py_temp\\\n",
      "\n",
      "\n",
      "요청하신 데이터를 수집 중입니다\n",
      "\n",
      "\n",
      "잠시만 기다려 주세요~~~~~^^\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "텍스트 추출 시작 =================================================\n",
      "\n",
      "\n",
      " 블로그 데이터를 수집합니다==============\n",
      "================================================================================\n",
      "1.블로그주소:  https://blog.naver.com/hy820715/221514204265\n",
      "2.작성자 닉네임:  가치랩장입니다\n",
      "3.작성일자: 2019. 4. 15. 16:27\n",
      "4.블로그내용: \n",
      " 웹 크롤링 진짜 많이 재미있죠?웹 크롤링에 대한 다양한 예제는 바로 이 책에 들어 있어요~   이 책안에는 다양한 유형의 웹사이트를 파이썬과 셀레니움을 활용하여 수집하는 노하우들이 다 들어 있습니다~하나씩 하나씩 따라하다 보면 금방 실력자가 되어 있으실 거예요~~^^그리고 수집된 데이터를 분석할 때  파이썬도 많이 사용하지만 R 프로그램도 많이 사용합니다~R 프로그램을 공부하시려면 아래의 책을 추천해드려요~~   위의 책에는 R 프로그램을 사용하여 텍스트 데이터를 분석하는 다양한 방법들부터 R 프로그램을 활용한 시각화 ,  지도 작업 , 정형 데이터를 핸들링하는 다양한 패키지 설명까지 제공되고 있어서 쉽고 빠르게 R 프로그램 사용 방법을 배우실 거예요~~​무엇보다도 절대로 포기하지 말고 열공하는 자세가 가장 중요합니다~~^^열공해 주세요~~^^​가치랩장 드림.​​​ \n",
      "\n",
      "\n",
      "https://postfiles.pstatic.net/MjAxOTA3MjlfMTQ0/MDAxNTY0MzcwMDYxNzYy.3iQRsQbrL8btKFujR9tFXCT9_PKu3DsiM6up1YBFhAcg.Yj62iO9BekL9OY8AnBmP2Fkc9kkfWbHOcPZh2rqf3k4g.JPEG.hy820715/완친파_표지_최종.jpg?type=w966\n",
      "1 -이미지 저장 완료\n",
      "https://postfiles.pstatic.net/MjAxOTA0MTVfNDgg/MDAxNTU1MzEzMDg3NTM1.biuF2sH30dCYq9ZITLLjl3rZNoanAYXM2dT4_0qS89sg.ttQz7rYhTwklVdSM_yO9UEbU5h35nO96W6DdR1OGXLQg.JPEG.hy820715/책표지_-_한국정보인재개발원_jpg.jpg?type=w966\n",
      "2 -이미지 저장 완료\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "총 소요시간은 7.2 초 입니다 \n",
      "파일 저장 완료: txt 파일명 : c:\\py_temp\\2022-03-26-08-12-24-블로그댓글수집\\2022-03-26-08-12-24-블로그댓글수집.txt \n",
      "파일 저장 완료: csv 파일명 : c:\\py_temp\\2022-03-26-08-12-24-블로그댓글수집\\2022-03-26-08-12-24-블로그댓글수집.csv \n",
      "파일 저장 완료: xls 파일명 : c:\\py_temp\\2022-03-26-08-12-24-블로그댓글수집\\2022-03-26-08-12-24-블로그댓글수집.xls \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 연습문제 : 네이버 블로그 정보 추출하여 다양한 형식의 파일로 저장하기\n",
    "# 연습용 블로그 : https://blog.naver.com/hy820715/221514204265\n",
    "print(\"=\" *80)\n",
    "print(\" 연습문제 : 네이버 블로그 상세 내역과 댓글 정보 추출하여 저장하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import numpy \n",
    "import pandas as pd  \n",
    "import random\n",
    "import os   \n",
    "import urllib.request\n",
    "import urllib\n",
    "\n",
    "#Step 2. 사용자로부터 필요한 정보 입력받기\n",
    "blog_url = input('1.크롤링할 블로그 주소를 입력하세요: ')\n",
    "query_txt1 = '블로그댓글수집'\n",
    "    \n",
    "f_dir = input(\"2.결과 파일을 저장할 폴더명만 쓰세요(예:c:\\\\py_temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir=\"c:\\\\py_temp\\\\\"\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"요청하신 데이터를 수집 중입니다\")\n",
    "print(\"\\n\")\n",
    "print(\"잠시만 기다려 주세요~~~~~^^\")\n",
    "print(\"\\n\")\n",
    "    \n",
    "# 저장될 파일위치와 이름을 지정합니다\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt1)\n",
    "os.chdir(f_dir+s+'-'+query_txt1)\n",
    "\n",
    "ff_name=f_dir+s+'-'+query_txt1+'\\\\'+s+'-'+query_txt1+'.txt'\n",
    "fc_name=f_dir+s+'-'+query_txt1+'\\\\'+s+'-'+query_txt1+'.csv'\n",
    "fx_name=f_dir+s+'-'+query_txt1+'\\\\'+s+'-'+query_txt1+'.xls'\n",
    "\n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "\n",
    "blog_addr2=[]\n",
    "w_name2=[]\n",
    "w_date2=[]\n",
    "blog_txt2=[]\n",
    "\n",
    "reple_1st_name=[]    # 1차 댓글 작성자이름\n",
    "reple_1st_txt=[]     # 1차 댓글 내용\n",
    "reple_1st_date=[]    # 1차 댓글 날짜\n",
    "\n",
    "reple_2nd_name=[]    # 2차 댓글 작성자이름\n",
    "reple_2nd_txt=[]     # 2차 댓글 내용\n",
    "reple_2nd_date=[]    # 2차 댓글 날짜\n",
    "\n",
    "\n",
    "gubun = re.split(\"[/]\",blog_url)\n",
    "\n",
    "if gubun[2] != 'blog.naver.com' :\n",
    "    print(\" 네이버 블로그만 가능합니다\")\n",
    "    \n",
    "else :\n",
    "    driver.get(blog_url)\n",
    "    time.sleep(random.randrange(2,5))  # 2 - 5 초 사이에 랜덤으로 시간 선택\n",
    "      \n",
    "    #Step 9. 각 블로그의 상세 결과를 출력하여 파일에 저장하기\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"텍스트 추출 시작 =================================================\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    #try: \n",
    "    driver.switch_to.frame('mainFrame')\n",
    "    #except:\n",
    "    #    continue\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    addr_1 = soup.select('#postViewArea')\n",
    "    addr_2 = soup.select('div[class=\"se-main-container\"]')\n",
    "       \n",
    "\n",
    "    if addr_1 :\n",
    "\n",
    "        f = open(ff_name, 'a',encoding='UTF-8')\n",
    "\n",
    "        print(\"=\" *80)             \n",
    "        print(\"1.블로그주소: \",blog_url)\n",
    "        blog_addr2.append(blog_url)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"블로그 데이터를 수집합니다==============\" + \"\\n\")\n",
    "\n",
    "        f.write(\"1.블로그 주소:\"+blog_url + \"\\n\")\n",
    "\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "            wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "            wname = \" \"\n",
    "        else :\n",
    "            wname = wname.replace(\"\\n\",\"\")\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        f.write(\"2.작성자 닉네임:\" + wname + \"\\n\")\n",
    "        w_name2.append(wname)\n",
    "\n",
    "        wdate = soup.select('p[class=\"date fil5 pcol2 _postAddDate\"]')\n",
    "        try : \n",
    "            wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "            wdate = ' '\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "        w_date2.append(wdate)\n",
    "\n",
    "        for i in addr_1:\n",
    "            for tag in soup.find_all('div',{'class':'se_component se_map default'}) :\n",
    "            #for tag in soup.find_all('div',{'class':'se_component se_oglink og_bSize '}) :\n",
    "                        tag.decompose()\n",
    "            for tag2 in soup.find_all('div',{'class':'se_component se_oglink default '}) :\n",
    "                        tag2.decompose()\n",
    "            blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "\n",
    "            try :\n",
    "                print(\"4.블로그내용: \\n\",blog_txt)\n",
    "            except UnicodeEncodeError :\n",
    "                continue\n",
    "\n",
    "            if len(blog_txt) > 32767 :\n",
    "                blog_txt = blog_txt[1:32760]\n",
    "\n",
    "            print(\"\\n\")\n",
    "            f.write(\"4.블로그 내용:\" + blog_txt + \"\\n\")\n",
    "            blog_txt2.append(blog_txt)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        f.close( )               \n",
    "\n",
    "    elif addr_2 :\n",
    "            \n",
    "        print(\" 블로그 데이터를 수집합니다==============\" )\n",
    "\n",
    "        f = open(ff_name, 'a',encoding='UTF-8')\n",
    "        print(\"=\" *80)    \n",
    "        print(\"1.블로그주소: \",blog_url)\n",
    "        blog_addr2.append(blog_url)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"블로그 데이터를 수집합니다==============\"  + \"\\n\")\n",
    "\n",
    "        f.write(\"1.블로그 주소:\"+blog_url + \"\\n\")\n",
    "\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "            wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "            wname = \" \"\n",
    "        else :\n",
    "            wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        f.write(\"2.작성자 닉네임:\" + wname + \"\\n\")\n",
    "        w_name2.append(wname)\n",
    "\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "            wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "            wdate = \" \"\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "        w_date2.append(wdate)\n",
    "\n",
    "        for i in addr_2:\n",
    "\n",
    "            map_1 = soup.select('div[class=\"se_component se_map default\"]')\n",
    "            map_2 = soup.select('div[class=\"se_component se_oglink og_bSize \"]')\n",
    "\n",
    "            if map_1 :\n",
    "                for tag in soup.find_all('div',{'class':'se_component se_map default'}) :\n",
    "                        tag.decompose()\n",
    "\n",
    "            elif map_2 :\n",
    "                for tag2 in soup.find_all('div',{'class':'se_component se_oglink og_bSize '}) :\n",
    "                        tag2.decompose()               \n",
    "\n",
    "            blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "\n",
    "            # 블로그 내용 출력중에 UnicodeEncodeError: 'UCS-2' codec can't encode characters.. 라는\n",
    "            # 에러가 나올 경우가 있다. 이럴 때를 대비해서 예외처리를 해야 한다\n",
    "            try :\n",
    "                print(\"4.블로그내용: \\n\",blog_txt)\n",
    "            except UnicodeEncodeError :\n",
    "                continue\n",
    "\n",
    "            if len(blog_txt) > 32767 :\n",
    "                blog_txt = blog_txt[1:32760]\n",
    "\n",
    "            print(\"\\n\")\n",
    "            f.write(\"4.블로그 내용:\" + blog_txt + \"\\n\")\n",
    "            blog_txt2.append(blog_txt)\n",
    "\n",
    "\n",
    "            # 이미지 저장\n",
    "            img_no = 1\n",
    "            img_src1 = soup.find('div','se-main-container').find_all('img')\n",
    "            for img in img_src1 :\n",
    "                img_src2 = img['src']\n",
    "                print(img_src2)\n",
    "                urllib.request.urlretrieve(urllib.parse.quote(img_src2.encode('utf8'), '/:'),str(img_no)+'.jpg')\n",
    "                                        \n",
    "                print(img_no , '-이미지 저장 완료')\n",
    "                img_no += 1\n",
    "\n",
    "        f.close()          \n",
    "\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"=\" *100)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"파일 저장 완료: txt 파일명 : %s \" %ff_name)\n",
    "print(\"파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *100)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
