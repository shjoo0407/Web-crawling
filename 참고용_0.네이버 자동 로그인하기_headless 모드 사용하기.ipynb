{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 자동 로그인하기\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "#크롬 드라이버 설정 및 웹 페이지 열기\n",
    "import chromedriver_autoinstaller\n",
    "chromedriver_autoinstaller.install()\n",
    "driver= webdriver.Chrome()\n",
    "\n",
    "#사용자에게 필요한 정보들을를 입력 받습니다.\n",
    "v_id = input('네이버 로그인 ID를 입력하세요:')\n",
    "v_passwd = input('네이버 로그인 비밀번호를 입력하세요:')\n",
    "time.sleep(2)\n",
    "\n",
    "# 네이버 자동 로그인\n",
    "driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "\n",
    "driver.execute_script(\"document.getElementsByName('id')[0].value=\\'\"+v_id+\"\\'\")\n",
    "driver.execute_script(\"document.getElementsByName('pw')[0].value=\\'\"+v_passwd+\"\\'\")\n",
    "driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/input').click()\n",
    "time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headless mode 활용하기\n",
    "# 대한민국 구석구석 사이트 상세 정보 수집하기\n",
    "\n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time          \n",
    "import pandas as pd    \n",
    "import math\n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 대한민국 구석구석 사이트 정보 수집용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.정보를 수집할 키워드는 무엇입니까?: ')\n",
    "\n",
    "cnt = int(input('2.몇 건의 정보를 수집할까요? :'))\n",
    "page_cnt = math.ceil( cnt / 10 )\n",
    "print('총 %s 건의 정보를 수집하기 위해 %s 페이지까지 이동할 예정입니다' %(cnt , page_cnt))\n",
    "\n",
    "#Step 3. 수집된 데이터를 저장할 폴더 이름 입력받기 \n",
    "f_dir = input(\"3.파일을 저장할 폴더명만 쓰세요(기본값:c:\\\\temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir=\"c:\\\\temp\\\\\"\n",
    "    \n",
    "print(\"\\n\")    \n",
    "\n",
    "#Step 3. 검색어 입력한 후 검색하여 해당 장르로 이동하기\n",
    "import chromedriver_autoinstaller\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "\n",
    "#headless 모드 설정하기\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "# options.add_argument('window-size=1920x1080')\n",
    "# options.add_argument(\"disable-infobars\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "#driver= webdriver.Chrome()\n",
    "\n",
    "query_url = 'https://korean.visitkorea.or.kr/'\n",
    "\n",
    "driver.get(query_url)\n",
    "time.sleep(2)\n",
    "driver.maximize_window()\n",
    "\n",
    "s_time = time.time( )\n",
    "element = driver.find_element_by_id(\"inp_search\")\n",
    "driver.find_element_by_id(\"inp_search\").click( )\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Step 5. 본문 내역 추출하기\n",
    "time.sleep(0.2)\n",
    "s_time = time.time( ) \n",
    "\n",
    "no2=[]           # 게시글 번호 컬럼\n",
    "title2=[ ]       # 게시글 제목 컬럼\n",
    "org2=[]          # 지자체명 컬럼\n",
    "content2=[ ]     # 본문 내용 컬럼\n",
    "tag2=[ ]         # 해시태그 컬럼\n",
    "\n",
    "no = 1           # 게시글 번호 초기값\n",
    "\n",
    "for a in range(1,page_cnt+1) :\n",
    "    print('')\n",
    "    print('현재 %s 페이지에 있는 정보를 수집합니다' %a)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content_list = soup.find('ul','list_thumType type1').find_all('li')\n",
    "    cnt_no = 1\n",
    "    \n",
    "    for b in content_list:\n",
    "        \n",
    "        try :\n",
    "            title = b.find('div','area_txt').find('div','tit').get_text().strip()\n",
    "        except :\n",
    "            continue\n",
    "        else :\n",
    "            # 1. 게시글 번호\n",
    "            print(\"\\n\")\n",
    "            print(\"%s 번째 정보를 추출하고 있습니다============\" %no)\n",
    "            no2.append(no)\n",
    "            print(\"1.번호 : %s\" %no)\n",
    "\n",
    "            # 2. 게시글 제목\n",
    "            title2.append(title)\n",
    "            print(\"2.제목 : %s\" %title )\n",
    "            \n",
    "            # 3. 지자체이름\n",
    "            org = b.find('div','service').find('p').get_text().strip()\n",
    "            org2.append(org)\n",
    "            print(\"3.지자체이름: %s\" %org)\n",
    "            \n",
    "            #4. 해시태그\n",
    "            try :\n",
    "                tag = b.find('p','tag_type').get_text().replace(\"#\",\" #\")\n",
    "            except :\n",
    "                tag = ' '\n",
    "                \n",
    "            tag2.append(tag)\n",
    "            print(\"4.해시태그: %s\" %tag)\n",
    "            \n",
    "            #5. 본문\n",
    "            \n",
    "            driver.find_element_by_xpath('//*[@id=\"listBody\"]/ul/li[%s]/div[2]/div[1]/a' %cnt_no).click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "            html_2 = driver.page_source\n",
    "            soup_2 = BeautifulSoup(html_2, 'html.parser')\n",
    "            \n",
    "            content_1 = soup_2.find('div','inr_wrap')\n",
    "            content_2 = soup_2.find('div','box_txtPhoto')\n",
    "            \n",
    "            if content_1 :\n",
    "                content = content_1.find('div','inr').find('p').get_text()\n",
    "                content2.append(content)\n",
    "                print('5.본문내용:', content)\n",
    "            elif content_2 :\n",
    "                content = soup_2.find('div','box_txtPhoto').get_text()\n",
    "                content2.append(content)\n",
    "                print('5.본문내용:', content)            \n",
    "            \n",
    "            driver.back()\n",
    "            \n",
    "            time.sleep(5)\n",
    "                       \n",
    "            no += 1         # 전체 게시글 번호용 값\n",
    "                       \n",
    "            if a == 1 and cnt_no == 4 : # 각 페이지별 게시글 번호용 값\n",
    "                cnt_no += 2   \n",
    "            else :\n",
    "                cnt_no += 1    \n",
    "            \n",
    "            if no > cnt :\n",
    "                 break\n",
    "                \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "    a += 1\n",
    "    try :\n",
    "        driver.find_element_by_link_text('%s' %a).click()\n",
    "    except :\n",
    "        driver.find_element_by_link_text('다음').click()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Step 6. 결과 저장하기\n",
    "import time\n",
    "import os\n",
    "\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' %(n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+'대한민국구석구석'+'-'+s+'-'+query_txt)\n",
    "\n",
    "fc_name = f_dir+'대한민국구석구석'+'-'+s+'-'+query_txt+'\\\\'+'대한민국구석구석'+'-'+s+'-'+query_txt+'.csv'\n",
    "fx_name = f_dir+'대한민국구석구석'+'-'+s+'-'+query_txt+'\\\\'+'대한민국구석구석'+'-'+s+'-'+query_txt+'.xls'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['번호']=no2\n",
    "df['제목']=pd.Series(title2)\n",
    "df['지자체명']=pd.Series(org2)\n",
    "df['본문내용']=pd.Series(content2)\n",
    "df['해시태그']=pd.Series(tag2)\n",
    "\n",
    "# xls , csv 형태로 저장하기\n",
    "df.to_excel(fx_name,index=False, encoding=\"utf-8\")\n",
    "df.to_csv(fc_name,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "e_time = time.time( )     \n",
    "t_time = e_time - s_time\n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"=\" *80)\n",
    "print(\"크롤링을 요청한 총 %s 건 중에서 %s 건의 데이터를 수집 완료 했습니다\" %(cnt,no-1))\n",
    "print('xls 저장경로: ',fx_name)\n",
    "print('csv 저장경로: ',fc_name)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"=\" *80)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
