{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " 이 크롤러는 RISS 사이트의 논문 및 학술자료 수집용 웹크롤러입니다.\n",
      "====================================================================================================\n",
      "1.수집할 자료의 키워드는 무엇입니까?: 전염병\n",
      "2.파일을 저장할 폴더명만 쓰세요(기본값:c:\\py_temp\\):\n",
      "검색하신 키워드 전염병 (으)로 총 1,447 건의 학위논문이 검색되었습니다\n",
      "이 중에서 몇 건을 수집하시겠습니까?: 5\n",
      "5 건의 데이터를 수집하기 위해 1 페이지의 게시물을 조회합니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 페이지 내용 수집 시작합니다 =======================\n",
      "\n",
      "\n",
      "1 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 1\n",
      "2.제목 : 코로나(COVID-19) 등 호흡기전염병 병원 내 감염 예방을 위한 표준개발\n",
      "3.작성자 : 황창의\n",
      "4.소속기관 : 계명대학교대학원\n",
      "5.발표년도 : 2021\n",
      "6.학위여부 : 국내박사\n",
      "7.초록내용 : 예로부터 전염병은 인류의 건강을 위협해 왔다.2000년대 이래 5차례 유행한 전염병이 발생하였다. 이러한 전염병은 발생되는 간격이 점점 짧아지고 발생 지속시간은 점점 길어지고 있었다. ...\n",
      "8.논문 URL 주소: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=be54d9b8bc7cdb09&control_no=aea8ae1ec75c689effe0bdc3ef48d419&keyword=전염병\n",
      "\n",
      "\n",
      "2 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 2\n",
      "2.제목 : 전염병에 대한 기독교 견해와 고찰\n",
      "3.작성자 : 이성훈\n",
      "4.소속기관 : 칼빈대학교 신학대학원\n",
      "5.발표년도 : 2021\n",
      "6.학위여부 : 국내석사\n",
      "7.초록내용 : 초록이 없습니다\n",
      "8.논문 URL 주소: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=be54d9b8bc7cdb09&control_no=53df85fac8ad6dc3ffe0bdc3ef48d419&keyword=전염병\n",
      "\n",
      "\n",
      "3 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 3\n",
      "2.제목 : 전염병 위험지각이 관광행동의도에 미치는 영향\n",
      "3.작성자 : 김혜진\n",
      "4.소속기관 : 세종대학교 대학원\n",
      "5.발표년도 : 2021\n",
      "6.학위여부 : 국내박사\n",
      "7.초록내용 : 관광은 관광객에게 재화나 서비스의 제공을 기반으로 하는 것으로 국가 및 지역의 경제·사회·문화·환경에 기여도가 높은 고부가가치 산업이며, 글로벌 규모의 사업이다. 그러나 2019년 중�...\n",
      "8.논문 URL 주소: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=be54d9b8bc7cdb09&control_no=e4f4a70463ace403ffe0bdc3ef48d419&keyword=전염병\n",
      "\n",
      "\n",
      "4 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 4\n",
      "2.제목 : 국제보건규칙 개정에 따른 우리나라 검역전염병 지정방안\n",
      "3.작성자 : 김윤호\n",
      "4.소속기관 : 경북대학교 보건대학원\n",
      "5.발표년도 : 2007\n",
      "6.학위여부 : 국내석사\n",
      "7.초록내용 : 국제보건규칙 개정에 따라 우리나라 검역전염병의 수정이 필요한 시점에 우리나라의 검역전염병의 지정근거를 제시하기 위하여 우리나라와 세계주요국의 검역전염병, 해외유입전염병 중 ...\n",
      "8.논문 URL 주소: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=be54d9b8bc7cdb09&control_no=24baac5cbfcf168bffe0bdc3ef48d419&keyword=전염병\n",
      "\n",
      "\n",
      "5 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 5\n",
      "2.제목 : 전염병과 펜데믹 상황에서 메디컬 처치의 운영에 관한 연구\n",
      "3.작성자 : 이재훈\n",
      "4.소속기관 : 칼빈대학교 신학대학원\n",
      "5.발표년도 : 2021\n",
      "6.학위여부 : 국내석사\n",
      "7.초록내용 : 초록이 없습니다\n",
      "8.논문 URL 주소: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=be54d9b8bc7cdb09&control_no=2b51133ed1f648a9ffe0bdc3ef48d419&keyword=전염병\n",
      "요청하신 작업이 모두 완료되었습니다\n",
      "요청하신 데이터 수집 작업이 정상적으로 완료되었습니다\n"
     ]
    }
   ],
   "source": [
    "# Chap 15.riss.kr 에서 특정 키워드로 논문 / 학술 자료 검색하기\n",
    "\n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 RISS 사이트의 논문 및 학술자료 수집용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.수집할 자료의 키워드는 무엇입니까?: ')\n",
    "\n",
    "#Step 3. 수집된 데이터를 저장할 파일 이름 입력받기 \n",
    "f_dir = input(\"2.파일을 저장할 폴더명만 쓰세요(기본값:c:\\\\py_temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir=\"c:\\\\py_temp\\\\\"\n",
    "\n",
    "#Step 4. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'http://www.riss.kr/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 5. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'query')\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "#Step 6.학위 논문 선택하기\n",
    "driver.find_element(By.LINK_TEXT,'학위논문').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 7.Beautiful Soup 로 본문 내용만 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "html_1 = driver.page_source\n",
    "soup_1 = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "#Step 8. 총 검색 건수를 보여주고 수집할 건수 입력받기\n",
    "import math\n",
    "total_cnt = soup_1.find('div','searchBox pd').find('span','num').get_text()\n",
    "print('검색하신 키워드 %s (으)로 총 %s 건의 학위논문이 검색되었습니다' %(query_txt,total_cnt))\n",
    "cnt = int(input('이 중에서 몇 건을 수집하시겠습니까?: '))\n",
    "page_cnt = math.ceil(cnt / 10)\n",
    "print('%s 건의 데이터를 수집하기 위해 %s 페이지의 게시물을 조회합니다.' %(cnt,page_cnt))\n",
    "print(\"\\n\")\n",
    "\n",
    "#Step 9. 데이터 수집하기\n",
    "no2=[]           # 게시글 번호 컬럼\n",
    "title2=[ ]       # 게시글 제목 컬럼\n",
    "author2=[]       # 논문 저자 컬럼\n",
    "company2=[ ]     # 소속 기관 컬럼\n",
    "date2=[ ]        # 게시글 날짜 컬럼\n",
    "suksa2=[ ]       # 국내석사 컬럼\n",
    "contents2=[]     # 초록내용\n",
    "full_url2=[]     # 논문 원본 URL\n",
    "\n",
    "no = 1           # 게시글 번호 초기값\n",
    "            \n",
    "for a in range(1,page_cnt+1) :\n",
    "    print(\"\\n\")\n",
    "    print(\"%s 페이지 내용 수집 시작합니다 =======================\" %a)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content_list = soup.find('div','srchResultListW').find_all('li')\n",
    "\n",
    "    for i in content_list:\n",
    "        # 논문 제목 체크하기\n",
    "        try:\n",
    "            title=i.find('p','title').get_text().strip()\n",
    "        except :\n",
    "            continue \n",
    "        else :\n",
    "            # 1.게시글 번호\n",
    "            print(\"\\n\")\n",
    "            print(\"%s 번째 정보를 추출하고 있습니다============\" %no)\n",
    "            no2.append(no)\n",
    "            print(\"1.번호 : %s\" %no)\n",
    "            \n",
    "            # 2. 논문 제목\n",
    "            title2.append(title.strip())\n",
    "            print(\"2.제목 : %s\" %title.strip())\n",
    "\n",
    "            # 3. 작성자\n",
    "            try :\n",
    "                author=i.find('p','etc').find('span','writer').get_text().strip()\n",
    "            except :\n",
    "                author = '작성자가 없습니다'\n",
    "                print(\"3.작성자 : %s\" %author.strip())\n",
    "                author2.append(author.strip())\n",
    "            else :\n",
    "                author2.append(author.strip())\n",
    "                print(\"3.작성자 : %s\" %author.strip())\n",
    "\n",
    "            # 4. 소속기관\n",
    "            try :\n",
    "                company=i.find('p','etc').find('span','assigned').get_text().strip()\n",
    "            except :\n",
    "                company='소속 기관이 없습니다'\n",
    "                company2.append(company.strip())\n",
    "                print(\"4.소속기관 : %s\" %company.strip())\n",
    "            else :\n",
    "                company2.append(company.strip())\n",
    "                print(\"4.소속기관 : %s\" %company.strip())\n",
    "\n",
    "            # 5. 발표날짜\n",
    "            try :\n",
    "                date_1 =i.find('p','etc').find_all('span')\n",
    "                date_2 = date_1[2].get_text().strip()\n",
    "            except :\n",
    "                date_2='발표날짜가 없습니다'\n",
    "                date2.append(date_2)\n",
    "                print(\"5.발표년도 : %s\" %date_2)\n",
    "            else :\n",
    "                date2.append(date_2)\n",
    "                print(\"5.발표년도 : %s\" %date_2)\n",
    "\n",
    "            # 6.학위여부\n",
    "            try :\n",
    "                suksa_1 =i.find('p','etc').find_all('span')\n",
    "                suksa_2 = suksa_1[3].get_text().strip()\n",
    "            except :\n",
    "                suksa_2='학위가 없습니다'\n",
    "                suksa2.append(suksa_2)\n",
    "                print(\"6.학위여부 : %s\" %suksa_2)\n",
    "            else :\n",
    "                suksa2.append(suksa_2)\n",
    "                print(\"6.학위여부 : %s\" %suksa_2)\n",
    "\n",
    "            # 7.초록 내용-해당 논문의 상세 내역에서 추출할 수 있음.    \n",
    "            url_1 = i.find('p','title').find('a')['href']\n",
    "            full_url = 'http://www.riss.kr'+url_1\n",
    "            time.sleep(1)\n",
    "            driver.get(full_url)\n",
    "\n",
    "            html_1 = driver.page_source\n",
    "            soup_1 = BeautifulSoup(html_1, 'html.parser')  \n",
    "            try :\n",
    "                cont=soup_1.find(\"div\",\"text\").find('p').get_text().replace(\"\\n\",\"\").strip()\n",
    "            except :\n",
    "                cont='초록이 없습니다'\n",
    "                contents2.append(cont)\n",
    "                print(\"7.초록내용 : %s\" %cont)\n",
    "            else :\n",
    "                contents2.append(cont)\n",
    "                print(\"7.초록내용 : %s\" %cont)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 8.논문 url 주소\n",
    "            full_url2.append(full_url)\n",
    "            print('8.논문 URL 주소:' , full_url)\n",
    "\n",
    "            driver.back()  # 이전 페이지로 돌아가기\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            no += 1\n",
    "            \n",
    "            if no > cnt :\n",
    "                break \n",
    "                            \n",
    "    a += 1 \n",
    "    b = str(a)\n",
    "\n",
    "    try :\n",
    "        driver.find_element(By.LINK_TEXT ,'%s' %b).click() \n",
    "    except :\n",
    "        driver.find_element(By.LINK_TEXT,'다음 페이지로').click()\n",
    "        \n",
    "print(\"요청하신 작업이 모두 완료되었습니다\")\n",
    "\n",
    "# Step 10. 수집된 데이터를 xls와 csv 형태로 저장하기\n",
    "# 현재 날짜와 시간으로 폴더 만들고 파일 이름 설정하기\n",
    "import os\n",
    "\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' %(n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+'RISS'+'-'+s+'-'+'학위논문')\n",
    "\n",
    "fc_name = f_dir+'RISS'+'-'+s+'-'+'학위논문'+'\\\\'+'RISS'+'-'+s+'-'+'학위논문'+'.csv'\n",
    "fx_name = f_dir+'RISS'+'-'+s+'-'+'학위논문'+'\\\\'+'RISS'+'-'+s+'-'+'학위논문'+'.xls'\n",
    "\n",
    "# 데이터 프레임 생성 후 xls , csv 형식으로 저장하기\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['번호']=no2\n",
    "df['제목']=pd.Series(title2)\n",
    "df['저자']=pd.Series(author2)\n",
    "df['소속(발행)기관']=pd.Series(company2)\n",
    "df['날짜']=pd.Series(date2)\n",
    "df['학위(논문일경우)']=pd.Series(suksa2)\n",
    "df['초록(논문일경우)']=pd.Series(contents2)\n",
    "df['자료URL주소']=pd.Series(full_url2)\n",
    "\n",
    "# xls 형태로 저장하기\n",
    "df.to_excel(fx_name,index=False, encoding=\"utf-8\" , engine='openpyxl')\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "df.to_csv(fc_name,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
